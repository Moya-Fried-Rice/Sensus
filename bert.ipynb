{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries & Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love this app so much, I've been using Spoti...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Perfect</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Best all around music streaming app I have use...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are y'all fr gatekeeping the play button on so...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>This has been my go-to for listening to music ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>I just tried playing music on my playlist, but...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>Horrible app, almost never plays the songs I w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>usually a 5 star experience but since the last...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>Great. Luv Spotify better than any other music...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  score\n",
       "0                                              It's good      4\n",
       "1      I love this app so much, I've been using Spoti...      5\n",
       "2                                                Perfect      5\n",
       "3      Best all around music streaming app I have use...      5\n",
       "4      Are y'all fr gatekeeping the play button on so...      1\n",
       "...                                                  ...    ...\n",
       "49995  This has been my go-to for listening to music ...      2\n",
       "49996  I just tried playing music on my playlist, but...      1\n",
       "49997  Horrible app, almost never plays the songs I w...      1\n",
       "49998  usually a 5 star experience but since the last...      1\n",
       "49999  Great. Luv Spotify better than any other music...      5\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download and display\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\abcjv\\\\Downloads\\\\spotify_reviews.csv\", usecols=['content', 'score'])\n",
    "df.head(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>processed_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's good</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love this app so much, I've been using Spoti...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>love app much ive use spotify year different a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Perfect</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>perfect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Best all around music streaming app I have use...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>best around music stream app use family plan g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are y'all fr gatekeeping the play button on so...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>yall fr gatekeeping play button song</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  score  sentiment  \\\n",
       "0                                          It's good      4          2   \n",
       "1  I love this app so much, I've been using Spoti...      5          2   \n",
       "2                                            Perfect      5          2   \n",
       "3  Best all around music streaming app I have use...      5          2   \n",
       "4  Are y'all fr gatekeeping the play button on so...      1          0   \n",
       "\n",
       "                                   processed_content  \n",
       "0                                               good  \n",
       "1  love app much ive use spotify year different a...  \n",
       "2                                            perfect  \n",
       "3  best around music stream app use family plan g...  \n",
       "4               yall fr gatekeeping play button song  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add sentiment value\n",
    "\n",
    "def stars_to_sentiment(stars):\n",
    "    if stars <= 2:\n",
    "        return 0  # Negative\n",
    "    elif stars == 3:\n",
    "        return 1  # Neutral\n",
    "    else:\n",
    "        return 2  # Positive\n",
    "    \n",
    "if 'score' in df.columns:\n",
    "    df['sentiment'] = df['score'].apply(stars_to_sentiment)\n",
    "else:\n",
    "    print(\"Error: Column 'score' not found in DataFrame.\")\n",
    "\n",
    "# Contraction words\n",
    "contractions = {\n",
    "    \"isn't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"hadn't\": \"had not\", \"isn't\": \"is not\",\n",
    "    \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mightn't\": \"might not\",\n",
    "    \"mustn't\": \"must not\", \"needn't\": \"need not\", \"needn't\": \"need not\", \"shan't\": \"shall not\",\n",
    "    \"she'd\": \"she would\", \"she'll\": \"she will\", \"she's\": \"she is\", \"shouldn't\": \"should not\",\n",
    "    \"that's\": \"that is\", \"there's\": \"there is\", \"they'd\": \"they would\", \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\", \"they've\": \"they have\", \"wasn't\": \"was not\", \"weren't\": \"were not\",\n",
    "    \"what's\": \"what is\", \"what'll\": \"what will\", \"what're\": \"what are\", \"what've\": \"what have\",\n",
    "    \"where's\": \"where is\", \"where've\": \"where have\", \"who's\": \"who is\", \"who'll\": \"who will\",\n",
    "    \"who're\": \"who are\", \"who've\": \"who have\", \"why's\": \"why is\", \"why're\": \"why are\",\n",
    "    \"why've\": \"why have\", \"won't\": \"will not\", \"wouldn't\": \"would not\", \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\", \"you're\": \"you are\", \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "# Important words\n",
    "sentiment_important_words = {\n",
    "    \"not\", \"no\", \"very\", \"good\", \"bad\", \"excellent\", \"love\", \"hate\", \"great\", \"feel\", \"wish\", \"would\", \"should\"\n",
    "}\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to NOUN if unknown\n",
    "    \n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Expand contractions (assumes contractions dictionary is available)\n",
    "    for contraction, expansion in contractions.items():\n",
    "        text = text.replace(contraction, expansion)\n",
    "\n",
    "    # Remove punctuation using regular expressions\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Initialize the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Define English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Customize the stopwords list for sentiment analysis (add negations or important words)\n",
    "    stop_words = stop_words - sentiment_important_words  # Remove sentiment important words from stopwords\n",
    "\n",
    "    # POS tagging\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    # Apply lemmatization based on POS tags and filter out stopwords and single-letter words\n",
    "    lemmatized_tokens = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags\n",
    "        if word.isalnum() and word not in stop_words and len(word) > 1  # Filter single-letter words\n",
    "    ]\n",
    "\n",
    "    # Join the lemmatized tokens back into a single string\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Test the function\n",
    "df['processed_content'] = df['content'].apply(preprocess_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0    0.581073\n",
       "2    0.276124\n",
       "1    0.142803\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check class distribution\n",
    "df['sentiment'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train dataset into train, validation and test sets\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['processed_content'], df['sentiment'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=df['sentiment'])\n",
    "\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "# tokens_train = tokenizer.batch_encode_plus(\n",
    "#     train_text.tolist(),\n",
    "#     max_length = 25,\n",
    "#     pad_to_max_length=True,\n",
    "#     truncation=True\n",
    "# )\n",
    "\n",
    "# # tokenize and encode sequences in the validation set\n",
    "# tokens_val = tokenizer.batch_encode_plus(\n",
    "#     val_text.tolist(),\n",
    "#     max_length = 25,\n",
    "#     pad_to_max_length=True,\n",
    "#     truncation=True\n",
    "# )\n",
    "\n",
    "# # tokenize and encode sequences in the test set\n",
    "# tokens_test = tokenizer.batch_encode_plus(\n",
    "#     test_text.tolist(),\n",
    "#     max_length = 25,\n",
    "#     pad_to_max_length=True,\n",
    "#     truncation=True\n",
    "# )\n",
    "\n",
    "# Tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length=25,\n",
    "    padding='max_length',  # Replace pad_to_max_length=True\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# Tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length=25,\n",
    "    padding='max_length',  # Replace pad_to_max_length=True\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# Tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length=25,\n",
    "    padding='max_length',  # Replace pad_to_max_length=True\n",
    "    truncation=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert lists to tensors\n",
    "\n",
    "train_seq = torch.tensor(tokens_train[\"input_ids\"])\n",
    "train_mask = torch.tensor(tokens_train[\"attention_mask\"])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val[\"input_ids\"])\n",
    "val_mask = torch.tensor(tokens_val[\"attention_mask\"])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test[\"input_ids\"])\n",
    "test_mask = torch.tensor(tokens_test[\"attention_mask\"])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        \n",
    "        self.bert = bert \n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,3)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        \n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "      \n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "      \n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [0.57364998 2.33428424 1.20717564]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abcjv\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),lr = 1e-5)\n",
    "\n",
    "# Convert to NumPy array (ensure integer type)\n",
    "train_labels_np = np.array(train_labels, dtype=int)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(train_labels_np), y=train_labels_np)\n",
    "\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list of class weights to a tensor\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "# push to GPU\n",
    "weights = weights.to(device)\n",
    "\n",
    "# define the loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "  \n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "        \n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    " \n",
    "        sent_id, mask, labels = batch\n",
    "        \n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "    total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "      # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "      # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "    \n",
    "    print(\"\\nEvaluating...\")\n",
    "  \n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    \n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "        \n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            \n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 / 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 1.083\n",
      "Validation Loss: 1.064\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# defining epochs\n",
    "epochs = 1\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "# for each epoch\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\nEpoch {epoch + 1} / {epochs}')\n",
    "    \n",
    "    # Train model with progress bar\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training\", leave=False)):\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Get predictions and compute the loss\n",
    "        preds = model(sent_id, mask)\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients and update model\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    \n",
    "    # Evaluate model with progress bar\n",
    "    model.eval()\n",
    "    total_valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(tqdm(val_dataloader, desc=\"Evaluating\", leave=False)):\n",
    "            batch = [r.to(device) for r in batch]\n",
    "            sent_id, mask, labels = batch\n",
    "            \n",
    "            preds = model(sent_id, mask)\n",
    "            loss = cross_entropy(preds, labels)\n",
    "            total_valid_loss += loss.item()\n",
    "    \n",
    "    avg_valid_loss = total_valid_loss / len(val_dataloader)\n",
    "    \n",
    "    # Save the best model\n",
    "    if avg_valid_loss < best_valid_loss:\n",
    "        best_valid_loss = avg_valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # Append training and validation losses\n",
    "    train_losses.append(avg_train_loss)\n",
    "    valid_losses.append(avg_valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {avg_train_loss:.3f}')\n",
    "    print(f'Validation Loss: {avg_valid_loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "# model's performance\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_feedback(feedback):\n",
    "    # Tokenize and encode the input feedback\n",
    "    tokens = tokenizer.encode_plus(\n",
    "        feedback,\n",
    "        max_length=25,  # same max_length used during training\n",
    "        padding='max_length',  # pad to max length\n",
    "        truncation=True,  # truncate if necessary\n",
    "        return_tensors='pt'  # return as PyTorch tensors\n",
    "    )\n",
    "\n",
    "    # Extract the input ids and attention mask\n",
    "    input_ids = tokens['input_ids'].to(device)\n",
    "    attention_mask = tokens['attention_mask'].to(device)\n",
    "\n",
    "    # Make prediction using the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        probs = outputs.detach().cpu().numpy()\n",
    "\n",
    "    # Get the predicted class (0, 1, or 2)\n",
    "    prediction = np.argmax(probs, axis=1)\n",
    "\n",
    "    # Map the predicted class back to sentiment labels (if needed)\n",
    "    sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "    predicted_sentiment = sentiment_map[prediction[0]]\n",
    "\n",
    "    return predicted_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of testing with actual feedback\n",
    "feedback = \"I love this product, it's amazing!\"  # Example feedback text\n",
    "predicted_sentiment = predict_feedback(feedback)\n",
    "print(f\"Predicted sentiment: {predicted_sentiment}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
